# DAG Eval Benchmark

![status](https://img.shields.io/badge/status-research--in--progress-blue)
![license](https://img.shields.io/badge/license-MIT-green)
![python](https://img.shields.io/badge/python-3.10+-orange)

**Benchmarking deterministic, LLM, agentic, and hybrid evaluation architectures for workflow orchestration code**

A model-agnostic, objective benchmark and open leaderboard for automatic code evaluation systems (judges) for Airflow, Prefect, and Dagster pipelines.

---

## âœ¨ Motivation

Modern LLMs can review code â€” but:
- LLMs change rapidly
- Model leaderboards become obsolete
- Reliability matters more than raw model capability

This project benchmarks **evaluation architectures**, not models.

We answer: *What is the most reliable way to build an automated judge for orchestration code?*

---

## ğŸ§  Core Idea

We evaluate **judge systems**, not LLMs.

Each submission is an evaluation architecture:
- **Deterministic** (tools + rules)
- **Single-LLM judge**
- **Multi-agent judge**
- **Hybrid** (tools â†’ LLM filter)
- **Verifier / debate systems** (optional)

All are measured against objective ground truth.

---

## ğŸ— Benchmark Tasks

### Task A â€” Gate Prediction (Runtime Compliance)

Predict whether code passes platform-critical checks:
- `syntax_valid`
- `imports_resolve`
- `instantiates`
- `has_structure`

**Ground truth:** Runtime oracle that actually executes these checks.

*Why it matters:* These gates determine whether a DAG/Flow/Job is deployable.

### Task B â€” Issue Detection

Detect standardized issue categories:

**General code quality**
- syntax
- imports
- security
- complexity
- style
- naming
- documentation
- error_handling
- unused
- undefined
- best_practice

**Orchestrator-specific**
- orchestrator_structure
- orchestrator_config

**Ground truth:** 
- Primary â†’ mutation injection labels
- Secondary â†’ static tool ensemble

---

## ğŸ§¬ Ground Truth: Mutation Engine

We automatically inject known defects into pipelines.

This gives:
- Objective labels
- Perfect category mapping
- Orchestrator-specific truth
- No human annotation
- No "tool imitation" criticism

**Example mutations:**
- Remove DAG/Flow/Job definition
- Break imports
- Insert undefined variables
- Hardcode secrets
- Remove task dependencies
- Invalid schedules
- Bare except

---

## ğŸ“Š Metrics

### Gate prediction
- Accuracy / F1 (overall & per-gate)
- Critical false negative rate

### Issue detection
- Precision / Recall / F1
- Per-category performance
- Severity-aware scoring

### Operational metrics
- Cost (tokens / $)
- Latency
- Stability (variance across runs)

---

## ğŸ† Leaderboard Philosophy

We rank:

**Evaluator architectures** â€” not models.

This makes results:
- Durable
- Model-agnostic
- Scientifically meaningful

---

## ğŸ§ª Research Questions

**RQ1 â€” Gate reliability**  
How accurately can evaluators predict runtime compliance?

**RQ2 â€” Defect detection under objective labels**  
Which architecture best detects real defects with minimal false positives?

**RQ3 â€” Costâ€“qualityâ€“stability trade-offs**  
What sits on the Pareto frontier?

---

## ğŸ› Supported Orchestrators

- Apache Airflow
- Prefect
- Dagster

---

## ğŸ“‚ Project Structure

```
DAG_Eval_Benchmark/
â”‚
â”œâ”€â”€ benchmark/
â”‚   â”œâ”€â”€ runtime_oracle/        # Ground truth execution engine
â”‚   â”œâ”€â”€ mutation_engine/        # Defect injection system
â”‚   â”œâ”€â”€ datasets/               # Benchmark datasets
â”‚
â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ deterministic/          # Rule-based evaluators
â”‚   â”œâ”€â”€ single_llm/             # Single LLM judges
â”‚   â”œâ”€â”€ multi_agent/            # Multi-agent systems
â”‚   â”œâ”€â”€ hybrid/                  # Hybrid approaches
â”‚
â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ evaluator_output.json   # Output schema specification
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_benchmark.py        # Main entry point
â”‚
â”œâ”€â”€ results/                     # Benchmark results
â””â”€â”€ leaderboard/                 # Leaderboard data
```

---

## âš™ï¸ Evaluator Output Schema

All evaluators must emit:

```json
{
  "gates": {
    "syntax_valid": true,
    "imports_resolve": true,
    "instantiates": false,
    "has_structure": true
  },
  "issues": [
    {
      "category": "security",
      "severity": "critical",
      "message": "Hardcoded password detected",
      "location": "line 42",
      "confidence": 0.91
    }
  ]
}
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Install

```bash
git clone https://github.com/<your-username>/DAG_Eval_Benchmark.git
cd DAG_Eval_Benchmark

python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run the benchmark

```bash
python scripts/run_benchmark.py \
  --evaluator deterministic \
  --orchestrator airflow
```

### 3ï¸âƒ£ View results

Outputs include:
- JSON results
- CSV summaries
- Performance plots
- Leaderboard entry

---

## ğŸ§© Implementing a Custom Evaluator

1. Create a new directory: `evaluators/my_evaluator/`
2. Implement the evaluation function:

```python
def evaluate(file_path: str) -> EvaluatorOutput:
    # Your evaluation logic here
    ...
    return EvaluatorOutput(...)
```

3. Run your evaluator:

```bash
python scripts/run_benchmark.py --evaluator my_evaluator
```

---

## ğŸ“ˆ Leaderboard Metrics

Each submission reports:
- Gate accuracy
- Issue detection F1
- Stability score (variance across runs)
- Token usage
- Runtime latency

---

## ğŸ”¬ Reproducibility

Planned features:
- Fixed datasets with versioning
- Mutation metadata preservation
- Deterministic runtime oracle
- Containerized submissions for isolation

---

## ğŸ—º Roadmap

- [x] Runtime oracle (PCT gates)
- [x] Mutation engine
- [ ] Deterministic baseline
- [ ] Single-LLM judge
- [ ] Multi-agent judge
- [ ] Hybrid judge
- [ ] Stability runner
- [ ] Cost tracking
- [ ] Public leaderboard

---

## ğŸ“œ Intended Contribution

This benchmark enables:
- Research on automated code judges
- Reliable LLM evaluation systems
- Orchestration-specific code quality analysis

---

## ğŸ¤ Contributing

We welcome contributions in:
- New evaluator architectures
- New mutation types
- Additional orchestrator support
- Reproducibility improvements
- Documentation and examples

Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## â­ Citation

```bibtex
@misc{dag_eval_benchmark,
  title={Benchmarking Evaluation Architectures for Workflow Orchestration Code},
  author={DAG Eval Benchmark Contributors},
  year={2026},
  publisher={GitHub},
  url={https://github.com/<your-username>/DAG_Eval_Benchmark}
}
```

---

## ğŸ™ Acknowledgments

Built with inspiration from the LLM-as-a-judge research community and workflow orchestration tooling ecosystems.